{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b86ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101c4eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pravogovruNd</th>\n",
       "      <th>issuedByIPS</th>\n",
       "      <th>docdateIPS</th>\n",
       "      <th>docNumberIPS</th>\n",
       "      <th>doc_typeIPS</th>\n",
       "      <th>headingIPS</th>\n",
       "      <th>doc_author_normal_formIPS</th>\n",
       "      <th>signedIPS</th>\n",
       "      <th>statusIPS</th>\n",
       "      <th>actual_datetimeIPS</th>\n",
       "      <th>actual_datetime_humanIPS</th>\n",
       "      <th>is_widely_used</th>\n",
       "      <th>textIPS</th>\n",
       "      <th>classifierByIPS</th>\n",
       "      <th>keywordsByIPS</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102685017</td>\n",
       "      <td>Постановление Правительства Российской Федерации</td>\n",
       "      <td>28.02.2020</td>\n",
       "      <td>210</td>\n",
       "      <td>Постановление</td>\n",
       "      <td>О внесении изменений в Правила предоставления ...</td>\n",
       "      <td>Правительство Российской Федерации</td>\n",
       "      <td>Мишустин М.</td>\n",
       "      <td>Действует без изменений</td>\n",
       "      <td>1710728935.8167202</td>\n",
       "      <td>Mon Mar 18 05:28:55 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nПРАВИТЕЛЬСТВО РОССИЙСКОЙ ФЕДЕРАЦИИ\\n \\nПОСТ...</td>\n",
       "      <td>010.140.030.010.000$Отмена, изменение и дополн...</td>\n",
       "      <td>АКЦИОНЕРНЫЕ, БЮДЖЕТ, ВЗНОС, ВНЕШНЕЭКОНОМИЧЕСКА...</td>\n",
       "      <td>правительство российской федерации постановлен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605953582</td>\n",
       "      <td>Приказ Федеральной таможенной службы</td>\n",
       "      <td>23.08.2023</td>\n",
       "      <td>751</td>\n",
       "      <td>Приказ</td>\n",
       "      <td>О внесении изменений в Порядок проведения тамо...</td>\n",
       "      <td>Федеральная таможенная служба</td>\n",
       "      <td>nan</td>\n",
       "      <td>Действует без изменений</td>\n",
       "      <td>1710772322.8312817</td>\n",
       "      <td>Mon Mar 18 17:32:02 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nФЕДЕРАЛЬНАЯ ТАМОЖЕННАЯ СЛУЖБА \\n \\nПРИКАЗ\\n...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td></td>\n",
       "      <td>федеральная таможенная служба приказ москва  а...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102386309</td>\n",
       "      <td>Приказ Федеральной антимонопольной службы</td>\n",
       "      <td>22.12.2015</td>\n",
       "      <td>1347/15</td>\n",
       "      <td>Приказ</td>\n",
       "      <td>Об утверждении коэффициентов сезонности, приме...</td>\n",
       "      <td>Федеральная антимонопольная служба</td>\n",
       "      <td>Артемьев И.</td>\n",
       "      <td>Действует без изменений</td>\n",
       "      <td>1710441798.790414</td>\n",
       "      <td>Thu Mar 14 21:43:18 2024</td>\n",
       "      <td>1</td>\n",
       "      <td>ФЕДЕРАЛЬНАЯ АНТИМОНОПОЛЬНАЯ СЛУЖБА \\n ПРИКАЗ ...</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td></td>\n",
       "      <td>федеральная антимонопольная служба приказ моск...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102100389</td>\n",
       "      <td>Распоряжение Правительства Российской Федерации</td>\n",
       "      <td>12.10.2005</td>\n",
       "      <td>1654-р</td>\n",
       "      <td>Распоряжение</td>\n",
       "      <td>О передаче в государственную собственность Омс...</td>\n",
       "      <td>Правительство Российской Федерации</td>\n",
       "      <td>Фрадков М.</td>\n",
       "      <td>Действует без изменений</td>\n",
       "      <td>1710335188.5398128</td>\n",
       "      <td>Wed Mar 13 16:06:28 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nПРАВИТЕЛЬСТВО РОССИЙСКОЙ ФЕДЕРАЦИИ\\n \\nРАСП...</td>\n",
       "      <td>010.070.020.040.290$Омская область $030.030.01...</td>\n",
       "      <td>ВОЕННЫЙ, ОБЛАСТЬ, ПАМЯТНИК, ПЕРЕДАЧА, СОБСТВЕН...</td>\n",
       "      <td>правительство российской федерации распоряжени...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102466500</td>\n",
       "      <td>Постановление Правительства Российской Федерации</td>\n",
       "      <td>30.03.2018</td>\n",
       "      <td>352</td>\n",
       "      <td>Постановление</td>\n",
       "      <td>О внесении изменений в государственную програм...</td>\n",
       "      <td>Правительство Российской Федерации</td>\n",
       "      <td>Медведев Д.</td>\n",
       "      <td>Утратил силу</td>\n",
       "      <td>1710375877.834703</td>\n",
       "      <td>Thu Mar 14 03:24:37 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nПРАВИТЕЛЬСТВО РОССИЙСКОЙ ФЕДЕРАЦИИ\\n \\nПОСТ...</td>\n",
       "      <td>010.140.030.010.000$Отмена, изменение и дополн...</td>\n",
       "      <td>ГОСУДАРСТВЕННЫЙ, ИЗМЕНЕНИЕ, ПРОГРАММА, СРЕДА, ...</td>\n",
       "      <td>правительство российской федерации постановлен...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pravogovruNd                                       issuedByIPS  docdateIPS  \\\n",
       "0    102685017  Постановление Правительства Российской Федерации  28.02.2020   \n",
       "1    605953582             Приказ Федеральной таможенной службы   23.08.2023   \n",
       "2    102386309        Приказ Федеральной антимонопольной службы   22.12.2015   \n",
       "3    102100389   Распоряжение Правительства Российской Федерации  12.10.2005   \n",
       "4    102466500  Постановление Правительства Российской Федерации  30.03.2018   \n",
       "\n",
       "  docNumberIPS    doc_typeIPS  \\\n",
       "0          210  Постановление   \n",
       "1          751         Приказ   \n",
       "2      1347/15         Приказ   \n",
       "3       1654-р   Распоряжение   \n",
       "4          352  Постановление   \n",
       "\n",
       "                                          headingIPS  \\\n",
       "0  О внесении изменений в Правила предоставления ...   \n",
       "1  О внесении изменений в Порядок проведения тамо...   \n",
       "2  Об утверждении коэффициентов сезонности, приме...   \n",
       "3  О передаче в государственную собственность Омс...   \n",
       "4  О внесении изменений в государственную програм...   \n",
       "\n",
       "             doc_author_normal_formIPS    signedIPS                statusIPS  \\\n",
       "0   Правительство Российской Федерации  Мишустин М.  Действует без изменений   \n",
       "1       Федеральная таможенная служба           nan  Действует без изменений   \n",
       "2  Федеральная антимонопольная служба   Артемьев И.  Действует без изменений   \n",
       "3   Правительство Российской Федерации   Фрадков М.  Действует без изменений   \n",
       "4   Правительство Российской Федерации  Медведев Д.             Утратил силу   \n",
       "\n",
       "   actual_datetimeIPS  actual_datetime_humanIPS is_widely_used  \\\n",
       "0  1710728935.8167202  Mon Mar 18 05:28:55 2024              0   \n",
       "1  1710772322.8312817  Mon Mar 18 17:32:02 2024              0   \n",
       "2   1710441798.790414  Thu Mar 14 21:43:18 2024              1   \n",
       "3  1710335188.5398128  Wed Mar 13 16:06:28 2024              0   \n",
       "4   1710375877.834703  Thu Mar 14 03:24:37 2024              0   \n",
       "\n",
       "                                             textIPS  \\\n",
       "0   \\nПРАВИТЕЛЬСТВО РОССИЙСКОЙ ФЕДЕРАЦИИ\\n \\nПОСТ...   \n",
       "1   \\nФЕДЕРАЛЬНАЯ ТАМОЖЕННАЯ СЛУЖБА \\n \\nПРИКАЗ\\n...   \n",
       "2   ФЕДЕРАЛЬНАЯ АНТИМОНОПОЛЬНАЯ СЛУЖБА \\n ПРИКАЗ ...   \n",
       "3   \\nПРАВИТЕЛЬСТВО РОССИЙСКОЙ ФЕДЕРАЦИИ\\n \\nРАСП...   \n",
       "4   \\nПРАВИТЕЛЬСТВО РОССИЙСКОЙ ФЕДЕРАЦИИ\\n \\nПОСТ...   \n",
       "\n",
       "                                     classifierByIPS  \\\n",
       "0  010.140.030.010.000$Отмена, изменение и дополн...   \n",
       "1                                            UNKNOWN   \n",
       "2                                            UNKNOWN   \n",
       "3  010.070.020.040.290$Омская область $030.030.01...   \n",
       "4  010.140.030.010.000$Отмена, изменение и дополн...   \n",
       "\n",
       "                                       keywordsByIPS  \\\n",
       "0  АКЦИОНЕРНЫЕ, БЮДЖЕТ, ВЗНОС, ВНЕШНЕЭКОНОМИЧЕСКА...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  ВОЕННЫЙ, ОБЛАСТЬ, ПАМЯТНИК, ПЕРЕДАЧА, СОБСТВЕН...   \n",
       "4  ГОСУДАРСТВЕННЫЙ, ИЗМЕНЕНИЕ, ПРОГРАММА, СРЕДА, ...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  правительство российской федерации постановлен...  \n",
       "1  федеральная таможенная служба приказ москва  а...  \n",
       "2  федеральная антимонопольная служба приказ моск...  \n",
       "3  правительство российской федерации распоряжени...  \n",
       "4  правительство российской федерации постановлен...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\n",
    "        'dataset/data.parquet',\n",
    "        engine=\"pyarrow\"\n",
    "    )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b145cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49727 entries, 0 to 49999\n",
      "Data columns (total 16 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   pravogovruNd               49727 non-null  object\n",
      " 1   issuedByIPS                49727 non-null  object\n",
      " 2   docdateIPS                 49727 non-null  object\n",
      " 3   docNumberIPS               49727 non-null  object\n",
      " 4   doc_typeIPS                49727 non-null  object\n",
      " 5   headingIPS                 49727 non-null  object\n",
      " 6   doc_author_normal_formIPS  49727 non-null  object\n",
      " 7   signedIPS                  49727 non-null  object\n",
      " 8   statusIPS                  49727 non-null  object\n",
      " 9   actual_datetimeIPS         49727 non-null  object\n",
      " 10  actual_datetime_humanIPS   49727 non-null  object\n",
      " 11  is_widely_used             49727 non-null  object\n",
      " 12  textIPS                    49727 non-null  object\n",
      " 13  classifierByIPS            49727 non-null  object\n",
      " 14  keywordsByIPS              49727 non-null  object\n",
      " 15  text_clean                 49727 non-null  object\n",
      "dtypes: object(16)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7583f382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hedg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hedg/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99e964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove or replace empty\n",
    "df = df.dropna(subset=['textIPS'])\n",
    "\n",
    "df['classifierByIPS'] = df['classifierByIPS'].fillna('UNKNOWN')\n",
    "df['keywordsByIPS'] = df['keywordsByIPS'].fillna('')\n",
    "\n",
    "df = df.drop(columns=['taggedtextIPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32510bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()                                 \n",
    "    text = re.sub(r'\\s+', ' ', text)                    \n",
    "    text = re.sub(r'\\d+', '', text)                     \n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a0edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data (to lowercase, remove punctuation, numbers and many spaces)\n",
    "df['text_clean'] = df['textIPS'].apply(clean)\n",
    "df.to_parquet('dataset/data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and remove stopwords\n",
    "russian_stopwords = set(stopwords.words(\"russian\"))\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    tokens = word_tokenize(text, language='russian')\n",
    "    return [word for word in tokens if word not in russian_stopwords]\n",
    "\n",
    "# df['tokens'] = df['text_clean'].apply(tokenize_and_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a833dba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5926594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 of 17\n",
      "Processing chunk 2 of 17\n",
      "Processing chunk 3 of 17\n",
      "Processing chunk 4 of 17\n",
      "Processing chunk 5 of 17\n",
      "Processing chunk 6 of 17\n",
      "Processing chunk 7 of 17\n",
      "Processing chunk 8 of 17\n",
      "Processing chunk 9 of 17\n",
      "Processing chunk 10 of 17\n",
      "Processing chunk 11 of 17\n",
      "Processing chunk 12 of 17\n",
      "Processing chunk 13 of 17\n",
      "Processing chunk 14 of 17\n",
      "Processing chunk 15 of 17\n",
      "Processing chunk 16 of 17\n",
      "Processing chunk 17 of 17\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 3000\n",
    "total = len(df)\n",
    "\n",
    "for i in range(0, total, chunk_size):\n",
    "    print(f\"Processing chunk {i // chunk_size + 1} of {total // chunk_size + 1}\")\n",
    "    chunk = df.iloc[i:i+chunk_size].copy()\n",
    "    \n",
    "    # tokenize\n",
    "    chunk['tokens'] = chunk['text_clean'].apply(tokenize_and_filter)\n",
    "    \n",
    "    # save to disk\n",
    "    chunk_path = f\"ruslowod_tokens_chunk_{i//chunk_size}.parquet\"\n",
    "    chunk.to_parquet(chunk_path)\n",
    "    \n",
    "    # free memory\n",
    "    del chunk\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99d099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [morph.parse(token)[0].normal_form for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53f5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\".\")\n",
    "\n",
    "# get all token chunk files\n",
    "chunk_files = sorted(data_dir.glob(\"ruslowod_tokens_chunk_*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd4ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing chunk 1/17: ruslowod_tokens_chunk_0.parquet\n",
      "Lemmatizing chunk 2/17: ruslowod_tokens_chunk_1.parquet\n",
      "Lemmatizing chunk 3/17: ruslowod_tokens_chunk_10.parquet\n",
      "Lemmatizing chunk 4/17: ruslowod_tokens_chunk_11.parquet\n",
      "Lemmatizing chunk 5/17: ruslowod_tokens_chunk_12.parquet\n",
      "Lemmatizing chunk 6/17: ruslowod_tokens_chunk_13.parquet\n",
      "Lemmatizing chunk 7/17: ruslowod_tokens_chunk_14.parquet\n",
      "Lemmatizing chunk 8/17: ruslowod_tokens_chunk_15.parquet\n",
      "Lemmatizing chunk 9/17: ruslowod_tokens_chunk_16.parquet\n",
      "Lemmatizing chunk 10/17: ruslowod_tokens_chunk_2.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m chunk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(chunk_file)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# lemmatize\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmas\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmas\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/pandas/core/series.py:4935\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4801\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4802\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4807\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4808\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4810\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4811\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4926\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4927\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4928\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4935\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1422\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1502\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/pandas/core/base.py:925\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2999\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mlemmatize\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlemmatize\u001b[39m(tokens):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [morph\u001b[38;5;241m.\u001b[39mparse(token)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnormal_form \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlemmatize\u001b[39m(tokens):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mmorph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnormal_form \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/pymorphy2/analyzer.py:315\u001b[0m, in \u001b[0;36mMorphAnalyzer.parse\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    312\u001b[0m word_lower \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m analyzer, is_terminal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_units:\n\u001b[0;32m--> 315\u001b[0m     res\u001b[38;5;241m.\u001b[39mextend(\u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_lower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_terminal \u001b[38;5;129;01mand\u001b[39;00m res:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/pymorphy2/units/by_analogy.py:126\u001b[0m, in \u001b[0;36mUnknownPrefixAnalyzer.parse\u001b[0;34m(self, word, word_lower, seen_parses)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix, unprefixed_word \u001b[38;5;129;01min\u001b[39;00m word_splits(word_lower):\n\u001b[1;32m    124\u001b[0m     method \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m, prefix)\n\u001b[0;32m--> 126\u001b[0m     parses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict_analyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43munprefixed_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munprefixed_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_parses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fixed_word, tag, normal_form, score, methods_stack \u001b[38;5;129;01min\u001b[39;00m parses:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mis_productive():\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/pymorphy2/units/by_lookup.py:24\u001b[0m, in \u001b[0;36mDictionaryAnalyzer.parse\u001b[0;34m(self, word, word_lower, seen_parses)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mParse a word using this dictionary.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 24\u001b[0m para_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilar_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_lower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchar_substitutes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fixed_word, parses \u001b[38;5;129;01min\u001b[39;00m para_data:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# `fixed_word` is a word with proper substitute (e.g. ё) letters\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m para_id, idx \u001b[38;5;129;01min\u001b[39;00m parses:\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/dawg_python/dawgs.py:369\u001b[0m, in \u001b[0;36mBytesDAWG.similar_items\u001b[0;34m(self, key, replaces)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilar_items\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, replaces):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Returns a list of (key, value) tuples for all variants of ``key``\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in this DAWG according to ``replaces``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    unicode strings.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_similar_items\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplaces\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/dawg_python/dawgs.py:339\u001b[0m, in \u001b[0;36mBytesDAWG._similar_items\u001b[0;34m(self, current_prefix, key, index, replace_chars)\u001b[0m\n\u001b[1;32m    336\u001b[0m next_index \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m    337\u001b[0m b_replace_char, u_replace_char \u001b[38;5;241m=\u001b[39m replace_chars[b_step]\n\u001b[0;32m--> 339\u001b[0m next_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_replace_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_index:\n\u001b[1;32m    341\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m current_prefix \u001b[38;5;241m+\u001b[39m key[start_pos:word_pos] \u001b[38;5;241m+\u001b[39m u_replace_char\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/dawg_python/wrapper.py:64\u001b[0m, in \u001b[0;36mDictionary.follow_bytes\u001b[0;34m(self, s, index)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollows transitions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[0;32m---> 64\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint_from_byte\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/data/inno_courses/DLS/.venv/lib/python3.10/site-packages/dawg_python/wrapper.py:53\u001b[0m, in \u001b[0;36mDictionary.follow_char\u001b[0;34m(self, label, index)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfollow_char\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, index):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollows a transition\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 53\u001b[0m     offset \u001b[38;5;241m=\u001b[39m \u001b[43munits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_units\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     next_index \u001b[38;5;241m=\u001b[39m (index \u001b[38;5;241m^\u001b[39m offset \u001b[38;5;241m^\u001b[39m label) \u001b[38;5;241m&\u001b[39m units\u001b[38;5;241m.\u001b[39mPRECISION_MASK\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m units\u001b[38;5;241m.\u001b[39mlabel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_units[next_index]) \u001b[38;5;241m!=\u001b[39m label:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, chunk_file in enumerate(chunk_files):\n",
    "    print(f\"Lemmatizing chunk {i+1}/{len(chunk_files)}: {chunk_file.name}\")\n",
    "    \n",
    "    chunk = pd.read_parquet(chunk_file)\n",
    "\n",
    "    # lemmatize\n",
    "    chunk['lemmas'] = chunk['tokens'].apply(lemmatize)\n",
    "    chunk['lemmatized_text'] = chunk['lemmas'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    del chunk['lemmas']\n",
    "\n",
    "    output_file = chunk_file.with_name(chunk_file.name.replace(\"tokens\", \"lemmatized\"))\n",
    "    chunk.to_parquet(output_file, index=False)\n",
    "\n",
    "    # cleanup memory\n",
    "    del chunk\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c2ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing chunk 10/17: ruslowod_tokens_chunk_2.parquet\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m chunk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(chunk_file)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# lemmatize\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmas\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mlemmatize\u001b[49m)\n\u001b[1;32m      8\u001b[0m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmas\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatize' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, chunk_file in enumerate(chunk_files[9:], 9):\n",
    "    print(f\"Lemmatizing chunk {i+1}/{len(chunk_files)}: {chunk_file.name}\")\n",
    "    \n",
    "    chunk = pd.read_parquet(chunk_file)\n",
    "\n",
    "    # lemmatize\n",
    "    chunk['lemmas'] = chunk['tokens'].apply(lemmatize)\n",
    "    chunk['lemmatized_text'] = chunk['lemmas'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    del chunk['lemmas']\n",
    "\n",
    "    output_file = chunk_file.with_name(chunk_file.name.replace(\"tokens\", \"lemmatized\"))\n",
    "    chunk.to_parquet(output_file, index=False)\n",
    "\n",
    "    # cleanup memory\n",
    "    del chunk\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856daa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all ready chunks files\n",
    "PATTERN = \"ruslowod_lemmatized_chunk_*.parquet\" \n",
    "files = list(data_dir.glob(PATTERN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59172cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat by 4 files and save each as preprocessed\n",
    "for i in range(0, len(files), 4):\n",
    "    dfs = []\n",
    "    for f in files[i:i+4]:\n",
    "        df = pd.read_parquet(\n",
    "            f,\n",
    "            engine=\"pyarrow\"\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    df_full = pd.concat(dfs, ignore_index=True)\n",
    "    df_full.to_parquet(f'dataset/preproc_data{i//4}.parquet')\n",
    "    del dfs\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
